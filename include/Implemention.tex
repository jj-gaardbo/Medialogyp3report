\chapter{Implemention}
	\section{How the program works}
	This section describes how to set up the prototype, connect the camera, limitations of the program and all that jazz.
	\subsection{VR}
	Something about VR toolkit in Unity
	\section{DLLs and Unity}
		When discussing the initial idea at the semester workshop it was suggested that we implement the design in the Unity game engine\footnote{Unity's homepage \url{https://unity3d.com/}}, as it allows for high-level management of objects in space, and would reduce development time on the part of the implementation not concerned with image processing. Unity has a fair number of different options for performing image processing. However, as we would be creating the fiducial markers from scratch we could not use any preexisting software. We were already familiar with openCV, a fast library for image processing and computer vision, so developing using openCV made sense.\\\\
		One issue is that Unity uses C\# and openCV is only developed for C++, C, Python, and Java. The method used in this project to bridge this language barrier is to compile the C++ code as a DLL, or "Dynamic Linked Library", and import it into the C\# code. DLLs  cannot be executed in the way normal applications are, and are instead designed to be used by other applications. Because they can be complied from a host of different languages they are not limited to what C\# code can do. Another advantage is that they are compiled in machine code and are therefore very fast\footnote{How to Write Native Plugins For Unity \url{https://www.alanzucconi.com/2015/10/11/how-to-write-native-plugins-for-unity/}}. However, there are a number of disadvantages with this implementation that must be taken into consideration, and which have made the process of completing this project less smooth. As Unity and the plug-in do not have access to each other's code, passing an object between the two is difficult, for example. For the same reason, debugging crashes is tricky. During development of this project, one fatal bug consistently crashing the program happened in an unmanaged part of Unity, so Unity could not give an error message. The code was developed in C++ which had no console where a log could be printed. Therefore troubleshooting became the source of a lot of frustration.\\\\
		Importing a function into DLL is simple, as seen in Listing \ref{listing:dllExport}.
\begin{listing}[H]
\caption{How to: declare a function for DLL export}
\label{listing:dllExport}
\begin{minted}[frame=lines, framesep=2mm,baselinestretch=1.1,fontsize=\footnotesize,linenos]{cpp}
extern "C" __declspec( dllexport ) int MyFunc(long parm1);
\end{minted}
\end{listing}
\codeword{extern "C"} tells the compiler that the function name should not be changed when it is compiled, so that its name is known when it is being imported into Unity. \codeword{__declspec()} tells the linker to do something. In this case \codeword{dllexport}, which tells it that it should export a function to a DLL. Therefore \codeword{__declspec(dllimport)} will be used to import the function in Unity. A sample call might look like this\\
\begin{listing}[H]
	\caption{How to: declare a function for DLL import}
	\label{listing:dllImport}
	\begin{minted}[frame=lines,
	framesep=2mm,baselinestretch=1.1,fontsize=\footnotesize,linenos]{cpp}
[DllImport("unityOpencvPlugin", EntryPoint = "init")]
public static extern int Init(ref int outCameraWidth, ref int outCameraHeight);
	\end{minted}
\end{listing}
The name of the DLL file "unityOpencvPlugin" is given as the "entrypoint". Unity will then do a search for a function called \codeword{init} in the DLL "unityOpencvPlugin". It is important that the function name is not changed when exporting, or this process will fail. In line 2; a function is declared that will call the imported function. It is important to note that this function must have the same signature (return type and arguments). Unity cannot check whether or not this is true, so it will proceed as if it is the case. Signature compatibility is sensible and straightforward for primitive types like \codeword{int}, but for objects it becomes a bit more tricky. It requires the declaration of similar objects in both languages, with variables in the same order. Otherwise the data will not make sense when returned. As an example, we need to return the markers found after image processing. They are represented in a structure: 
\begin{listing}[H]
	\caption{Objects in C\# and C++}
	\label{listing:objects}
	\begin{minted}[frame=lines, framesep=2mm,baselinestretch=1.1,fontsize=\footnotesize,linenos]{c++}
			//in C++
struct ObjectData{
ObjectData(int x, int y, int type, int color) : X(x), Y(y), Type(type), Color(color) {}
int X, Y, Type, Color;
};
			//in C#	
[StructLayout(LayoutKind.Sequential, Size = 16)]
public struct CVObject{
public int X, Y, Type, Color;
}
	\end{minted}
\end{listing}
In Unity the structure needs to be laid out sequentially when exported to unmanaged memory.\footnote{Unity layout \url{https://msdn.microsoft.com/en-us/library/system.runtime.interopservices.layoutkind(v=vs.110).aspx}}. In order to detect several objects it is easier to pass a pointer to an array, and make C++ fill it out from the code. A few things are needed for this to work. The function needs to take a pointer as an argument, and the size of the array must be passed as an argument, so that the method does not overwrite data outside of the array. The memory that the pointer points to must not remain static, or the pointer would no longer point to the right data. In C\# the keyword \codeword{Fixed}Â prevents data from being moved in memory, and \codeword{unsafe} enables working with pointers. Below is an example from our implementation in the project where we pass a pointer to C++. %be careful with "unsafe" though. It oftentimes leads to children. 
\begin{listing}[H]
\caption{The function call to pass a pointer to C++, which is filled by the code}
\label{listing:pointer}
\begin{minted}[frame=lines,
		framesep=2mm,baselinestretch=1.1,fontsize=\footnotesize,linenos]{c++}
unsafe{
	fixed (CVObject* outMarkers = _markers){
		Cap(outMarkers, _maxMarkerDetectCount, ref detectedMarkersCount);
	}
}
\end{minted}
\end{listing}

\section{BGR to RG chromaticity conversion}
Its was found that differences in lighting conditions made thresholding the feed from the camera at best suboptimal. To circumvent this the feed is converted to rg chromaticity, also known as normalized rgb, and henceforth RG. RG contains no intensity information and as such is not a color space but instead a chromatic space\footnote{\url{https://en.wikipedia.org/wiki/Chromaticity}}.
\subsection{Theory of RG} \todo{maybe move to analysis}
The critical property that RG has is it does not record the individual channels intensity, but rather the color's intensity as a percentage of the total intensity. Mathematically the RG value of a channel can be derived from an RGB value in the following way.
\[ RG_x = \frac{RGB_x}{RGB_1 + RGB_2 + RGB_3}\]
where x represents which of the channels that is to be converted. This will give a value between 0 and 1 depending on how big a percentage of the total intensity comes from that color. However, since floats are operationally slow it is advised to scale the value to 8bits by multiplying by 255. If one where to convert all three, with capital letter representing the RG chromaticity, the channels can be found as.
\[ RED = \frac{red}{red + blue + green} * 255\]
\[ BLUE = \frac{blue}{red + blue + green} * 255\]
\[ GREEN = \frac{green}{red + blue + green} * 255\]
This means that in RG the sum of all channels must be 255.
\[ \frac{red}{red + green + blue} + \frac{green}{red + green + blue} \frac{blue}{red + green + blue} =  \frac{red + green + blue}{red + green + blue} = 1\]
It is observed that in RG the channel values are codependent. If the value of red is large, then the other channels must by definition contain small value as they all must add up to 255. A result of this is that one can find the value in one channel if the other two are known.
\[ BLUE = 255 - RED - GREEN\]
So it is possible to store the data using two bits, and calculate the third color if it is needed. This loss of information comes from the loss of intensity data\cite{NormRGB}. for example two colors (0,10,50) and (0,50,250) will produce exactly similar value when converted to RG. 
\[ BLUE = \frac{50}{50 + 10} * 255 = (int)212.5 = 212 \]
\[ BLUE = \frac{250}{250 + 50} * 255 = (int)212.5 = 212 \]
The same is true for any two color that are proportionately similar.\\
\todo{creat image showing (0,10,50) and the other, after conversion to (0,42,212)} 
In relation to image processing one has to be aware that noise, particularly in shadows, mess with the rg space. For this reason it is recommended to zero the values if the total sum is below a certain value.

\subsection{Thresholding in RG}
Because RG can be represented by two values it is possible to plot the color space using two axes. 
\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\linewidth]{figure/Analysis/normRGB.png}
	\caption{The entire RG color space is visible here, it can be represented in 2D due to the loss of intensity data that happens during conversion.}
	\label{fig:rgbNorm}
\end{figure}



\subsection{rg chomaticity in code}
The process of converting an rgb image described above is a matter of simple point processing . We have made a few changes to speed up the process at runtime. The current code performs these two steps at each pixel in the input image.\\
\begin{enumerate}
	\item Find the sum of the rgb chanels.
	\item Assign the corresponding pixel in the output to the value found in a look up table using the sum and value in the channel\\
\end{enumerate}

Really the only big change is the decision to use a look up table. A look up table is useful if you are going to be performing the same operation on similar data a lot of times. The concept is, "instead of calculating this thing every time, why don't i calculate a table with the results of all possible inputs and just look up what the result is". This might sound like madness until one realizes that a webcam often has a resolution of 640 x 480. That is 307200 pixel. Now mulitplications and additions are very fast and generally faster than memory access\todo{references for this}. Division is a more expensive operation and we have to do division and multiplication three times per pixel (one for each channel). We found that at runtime using a lookuptable and increased the speed of our conversion algorithm by 100.25\% as can be seen in listing \ref{rgConvSpeed}. 
\begin{center}
	\begin{table}[H]
		\centering
		\label{rgConvSpeed}
		\begin{tabular}{ l | l }
			\hline			
			No lookup table & .0015028 sec\\
			Look up if above threshold & .0010218 sec\\
			Look up table always& .0007741 sec\\
			\hline 
		\end{tabular}
	\caption{Time taken to run the function. It can be seen that the more of the data that is precomputed into a lookup table, the faster the function executes. Due to the number of repetitions even something as basic as a "if smaller than" statement can massively increase the time taken to run. The times were based the average times after 100000 runs on a test image with the dimensions 540 * 720}
	\end{table}
\end{center}\todo{show the normalized image}
Creating a look up table is straight forward. We know from the theory section that a color can be assigned by $RGcolor = 255 * \frac{color}{sumOfColor}$. There is two variables at play here. Sum which can vary between 0, when all colors are zero, and 765, and Color which can vary between 0, and 255. To compute a lookup table  that includes all possible combinations of variables, we need an array of size 766 * 256. We then loop through it, assigning values based on the two inputs, as can be seen listing \ref{listing:lutTable}.\\
\begin{listing}[H]
\caption{Instantiating our lookup table}
\label{listing:lutTable}
\begin{minted}[frame=lines,
	framesep=2mm,baselinestretch=1.1,fontsize=\footnotesize,linenos]{c++}
	int divLUT[766][256]; //division lookuptavle;
	for (int i = 0; i < 766; i++) {
		for (int j = 0; j < 256; j++) {
			if (i < rgConvThreshold) { 
				divLUT[i][j] = 0;
			}
			else {
				divLUT[i][j] = (j * 255) / i;
			}
		}
	}
\end{minted}
\end{listing}
The first thing the loop does is to check if we are below the threshold where we should zero it(to prevent noise in dark areas from messing with the image). If it is we set the index to zero. Otherwise we calculate the value that would result from the color(j) and sum(i).
Now that the table has been made all we have to do is pass it to a function so it can use it.

\subsubsection{Summing RGB channels}

Really this is quiet simple and the code we use can be seen in listing \ref{listing:sum}. it should be mentioned that the image is loaded so that it has a uchar for each chanel. These are laid out continuously, so blue for the first pixel is right next to green for the first pixel. Before the loop we created an integer to hold the value. In the loop we get a pointer to the first pixel, the blue one. We add the value of it and the next two positions to sum. If the sum is below a certain threshold, the overall image is dark. This is bad in RG chromaticity, and so its red green and blue is set to zero. We then continue to the next pixel.
\begin{listing}[H]
	\caption{How to sum value in BGR using pointer}
	\label{listing:sum}
	\begin{minted}[frame=lines,
		framesep=2mm,baselinestretch=1.1,fontsize=\footnotesize,linenos]{c++}
	sum = p[j] + p[j + 1] + p[j + 2];
	if (sum < rgConvThreshold) {
		cp[j] = 0;
		cp[j + 1] = 0;
		cp[j + 2] = 0;
		continue;
	}
	\end{minted}
\end{listing}
\subsubsection{Assigning values in output using look up}
I mean this part really is not  magic given the layout of the Look up table all we need to do is. cp is a pointer to the corresponding pixels blue value, in output.
\begin{listing}[H]
	\caption{Assigning values}
	\label{listing:sum}
	\begin{minted}[frame=lines,
		framesep=2mm,baselinestretch=1.1,fontsize=\footnotesize,linenos]{c++}
		cp[j] = divLUT[sum][p[j]];
		cp[j+1] = divLUT[sum][p[j+1]];
		cp[j+2] = divLUT[sum][p[j+2]];
	\end{minted}
\end{listing}


\section{Limitations}
Or different section title. This section will describe how the implementation differs from the description of the original idea in the "Design" section. Why was it not possible to do X (e.g. live update or 1024 item types etc.) and what did we do instead?